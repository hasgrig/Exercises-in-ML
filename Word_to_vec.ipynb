{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hasgrig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.sparse.linalg import svds\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import scipy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"wiki-text.txt\", \"r\") \n",
    "Text = file.read()\n",
    "small_text = Text[0:500001]\n",
    "smaller_text = Text[0:10000]\n",
    "medium_text = Text[0:2000000]\n",
    "tiny_text = Text[0:99]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vocab(text, min_freq):\n",
    "    words = text.split(' ')\n",
    "    while '' in words:\n",
    "        words.remove('')\n",
    "    #remove words that occur less than 1000 times    \n",
    "    word_count = nltk.FreqDist(words)\n",
    "    word_count = {k:v for (k,v) in word_count.items() if v>min_freq}\n",
    "    #remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_count = {k:v for (k,v) in word_count.items() if k not in stop_words}\n",
    "    return word_count.keys(), words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_pair_matrix(wordlist, vocab):\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    keys = list(vocab)\n",
    "    values = list(range(len(vocab)))\n",
    "    dicti = dict(zip(keys, values))\n",
    "    for i in range(len(wordlist)):\n",
    "        if i % 10000 == 0:\n",
    "            clear_output()\n",
    "            print(len(wordlist), i)\n",
    "        for j in range(1,6):\n",
    "            if i+j<len(wordlist) and (wordlist[i] in vocab) and (wordlist[i+j] in vocab):\n",
    "                matrix[dicti[wordlist[i]]][dicti[wordlist[i+j]]]+=1\n",
    "        for j in range(1,6):    \n",
    "            if i-j>=0 and (wordlist[i] in vocab) and (wordlist[i-j] in vocab):\n",
    "                matrix[dicti[wordlist[i]]][dicti[wordlist[i-j]]]+=1\n",
    "        \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_M(pair_matrix):\n",
    "    M = np.zeros(pair_matrix.shape)\n",
    "    NS = np.sum(pair_matrix)\n",
    "    Nwi_vec = np.sum(pair_matrix, axis=1)\n",
    "    Nwj_vec = np.sum(pair_matrix, axis=0)\n",
    "    for i in range(pair_matrix.shape[0]):\n",
    "        if i%100==0:\n",
    "            clear_output()\n",
    "            print(pair_matrix.shape[0], i)\n",
    "        for j in range(pair_matrix.shape[1]):\n",
    "            if j>=i:\n",
    "                Nwij = pair_matrix[i][j]\n",
    "                Nwi = Nwi_vec[i]\n",
    "                Nwj = Nwj_vec[j]\n",
    "                M[i][j] = math.log(((Nwij+1)*NS)/(Nwi*Nwj))\n",
    "            else:\n",
    "                M[i][j] = M[j][i]\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab, wordlist = get_text_vocab(Text, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124301826 124300000\n"
     ]
    }
   ],
   "source": [
    "pair_matrix = get_word_pair_matrix(wordlist, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13201 13200\n"
     ]
    }
   ],
   "source": [
    "M = get_M(pair_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = svds(scipy.sparse.csr_matrix(M), k=50)\n",
    "S = np.diag(s)\n",
    "W = np.matmul(U, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(vocab)\n",
    "values = list(range(len(vocab)))\n",
    "dicti = dict(zip(keys, values))\n",
    "dist_matrix = pairwise_distances(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closes words to physics, republican, einstein, algebra, fish are ... \n",
      "\n",
      "['physics' 'mathematics' 'mathematical' 'sciences' 'analysis' 'quantum']\n",
      "['republican' 'presidential' 'secretary' 'senator' 'representative'\n",
      " 'democratic']\n",
      "['einstein' 'relativity' 'marx' 'astronomical' 'darwin' 'mathematicians']\n",
      "['algebra' 'finite' 'geometry' 'dimensional' 'notation' 'matrix']\n",
      "['fish' 'plants' 'wild' 'trees' 'animal' 'plant']\n"
     ]
    }
   ],
   "source": [
    "print(\"The closes words to physics, republican, einstein, algebra, fish are ... \\n\")\n",
    "print(np.asarray(list(vocab))[np.argsort(dist_matrix[dicti['physics']])[0:6]])\n",
    "print(np.asarray(list(vocab))[np.argsort(dist_matrix[dicti['republican']])[0:6]])\n",
    "print(np.asarray(list(vocab))[np.argsort(dist_matrix[dicti['einstein']])[0:6]])\n",
    "print(np.asarray(list(vocab))[np.argsort(dist_matrix[dicti['algebra']])[0:6]])\n",
    "print(np.asarray(list(vocab))[np.argsort(dist_matrix[dicti['fish']])[0:6]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogies(a1, a2, b1):\n",
    "    b2 = W[dicti[a2]]-W[dicti[a1]]+W[dicti[b1]]\n",
    "    dist_dif = W-b2\n",
    "    inds = np.argsort(np.linalg.norm(dist_dif, axis=1))[0:5]\n",
    "    return (np.asarray(list(vocab))[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : woman :: boy : ?\n",
      "['girls' 'boys' 'baby' 'broadway' 'cat']\n",
      "die : death :: live : ?\n",
      "['children' 'year' 'people' 'day' 'film']\n",
      "mother : father :: daughter : ?\n",
      "['son' 'daughter' 'father' 'wife' 'brother']\n"
     ]
    }
   ],
   "source": [
    "print(\"man : woman :: boy : ?\")\n",
    "print(get_analogies(\"man\", \"woman\", \"boy\"))\n",
    "print(\"die : death :: live : ?\")\n",
    "print(get_analogies(\"die\", \"death\", \"live\"))\n",
    "print(\"mother : father :: daughter : ?\")\n",
    "print(get_analogies(\"mother\", \"father\", \"daughter\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
